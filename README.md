# Simple Scraping Pipeline

A modern, scalable web scraping pipeline that extracts PDFs, Images, and YouTube videos based on keyword CSV files. Features a clean React UI and FastAPI backend with PostgreSQL database.

## Features

- ğŸ“„ **PDF Scraping** - Extract PDF documents using DuckDuckGo
- ğŸ–¼ï¸ **Image Scraping** - Extract images using Bing Image API
- ğŸ¥ **YouTube Scraping** - Extract YouTube videos using yt-dlp
- ğŸ“Š **Database Storage** - PostgreSQL with pgAdmin for data management
- ğŸ¨ **Modern UI** - Clean, professional React interface
- ğŸ“¥ **Bulk Downloads** - Download all items as ZIP (PDFs/Images) or CSV (YouTube)
- ğŸ” **Real-time Progress** - Live tracking of scraping progress
- âœ… **Duplicate Detection** - Prevents duplicate items
- ğŸ“ **Multi-file Support** - Upload multiple CSV files at once

## Tech Stack

- **Backend**: FastAPI (Python)
- **Frontend**: React + Vite
- **Database**: PostgreSQL (Docker)
- **Scraping**: DuckDuckGo, Bing Image API, yt-dlp

## Quick Start

### Prerequisites

- Docker Desktop
- Python 3.8+
- Node.js 16+ and npm

### Installation

1. **Clone the repository**
   ```bash
   git clone <repository-url>
   cd simple_pipeline
   ```

2. **Start Database**
   ```bash
   cd backend
   docker-compose up -d
   ```

3. **Setup Backend**
   ```bash
   cd backend
   python -m venv venv
   .\venv\Scripts\Activate.ps1  # Windows
   # or: source venv/bin/activate  # Linux/Mac
   pip install -r requirements.txt
   python init_db.py
   uvicorn app.main:app --reload --port 8001
   ```

4. **Setup Frontend**
   ```bash
   cd frontend
   npm install
   npm run dev
   ```

5. **Access the Application**
   - Frontend: http://localhost:5174
   - Backend API: http://localhost:8001
   - API Docs: http://localhost:8001/docs
   - pgAdmin: http://localhost:5050

## Usage

1. **Upload CSV Files**: Select one or more CSV files containing keywords (one keyword per line)
2. **Select Content Types**: Check PDF, Image, and/or YouTube checkboxes
3. **Start Scraping**: Click "Start Scraping" and monitor real-time progress
4. **Download Results**: Click "Download Scraped Data" to download all items

## Project Structure

```
simple_pipeline/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ config.py          # Configuration settings
â”‚   â”‚   â”œâ”€â”€ database.py         # Database models
â”‚   â”‚   â”œâ”€â”€ main.py            # FastAPI application
â”‚   â”‚   â”œâ”€â”€ models.py          # Pydantic models
â”‚   â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”‚   â””â”€â”€ scraping.py    # API endpoints
â”‚   â”‚   â””â”€â”€ scraper/
â”‚   â”‚       â”œâ”€â”€ base.py         # Base scraper class
â”‚   â”‚       â”œâ”€â”€ manager.py      # Scraper manager
â”‚   â”‚       â”œâ”€â”€ pdf_scraper.py  # PDF scraper
â”‚   â”‚       â”œâ”€â”€ image_scraper.py # Image scraper
â”‚   â”‚       â””â”€â”€ youtube_scraper.py # YouTube scraper
â”‚   â”œâ”€â”€ docker-compose.yml     # Docker services
â”‚   â”œâ”€â”€ requirements.txt      # Python dependencies
â”‚   â”œâ”€â”€ init_db.py            # Database initialization
â”‚   â””â”€â”€ clear_database.py     # Database clearing script
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.jsx           # Main React component
â”‚   â”‚   â”œâ”€â”€ index.css         # Styles
â”‚   â”‚   â””â”€â”€ main.jsx          # React entry point
â”‚   â””â”€â”€ package.json          # Node dependencies
â”‚
â”œâ”€â”€ keywords1.csv            # Sample input file 1
â”œâ”€â”€ keywords2.csv            # Sample input file 2
â”œâ”€â”€ SETUP_GUIDE.md          # Detailed setup instructions
â””â”€â”€ README.md               # This file
```

## Configuration

### Scraping Limits

Edit `backend/app/config.py`:
```python
MAX_RESULTS_PER_KEYWORD: int = 2  # Items per keyword
```

### Database

Default connection: `postgresql://postgres:postgres@localhost:5432/simple_scraping_db`

Edit `backend/app/config.py` to change database settings.

## API Endpoints

- `POST /api/scraping/upload-csv` - Upload CSV files and start scraping
- `GET /api/scraping/progress/{task_id}` - Get scraping progress
- `GET /api/scraping/items` - Get scraped items (filtered by task_id)
- `GET /api/scraping/download-bulk` - Download items as ZIP (PDF/Image)
- `GET /api/scraping/download-youtube-csv` - Download YouTube items as CSV
- `POST /api/scraping/clear-database` - Clear all items from database

## Troubleshooting

See `SETUP_GUIDE.md` for detailed troubleshooting steps.

## License

MIT License

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.
